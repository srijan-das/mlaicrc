{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLfMDlQcS+OLpxlDUH1bRB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srijan-das/mlaicrc/blob/master/PycharmProjects/Udemy/keras_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk1VWKlScmEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiZtJ3-mdK_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkFqx5pvdOEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a52df015-334f-4f79-a518-7ebd84ed2a06"
      },
      "source": [
        "df = pd.read_csv('/content/cancer_classification.csv')\n",
        "print(df.head())"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   mean radius  mean texture  ...  worst fractal dimension  benign_0__mal_1\n",
            "0        17.99         10.38  ...                  0.11890                0\n",
            "1        20.57         17.77  ...                  0.08902                0\n",
            "2        19.69         21.25  ...                  0.08758                0\n",
            "3        11.42         20.38  ...                  0.17300                0\n",
            "4        20.29         14.34  ...                  0.07678                0\n",
            "\n",
            "[5 rows x 31 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NSCmFMudq-C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "36b3b5c8-19a3-4915-865e-30bc49d88d33"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 31 columns):\n",
            "mean radius                569 non-null float64\n",
            "mean texture               569 non-null float64\n",
            "mean perimeter             569 non-null float64\n",
            "mean area                  569 non-null float64\n",
            "mean smoothness            569 non-null float64\n",
            "mean compactness           569 non-null float64\n",
            "mean concavity             569 non-null float64\n",
            "mean concave points        569 non-null float64\n",
            "mean symmetry              569 non-null float64\n",
            "mean fractal dimension     569 non-null float64\n",
            "radius error               569 non-null float64\n",
            "texture error              569 non-null float64\n",
            "perimeter error            569 non-null float64\n",
            "area error                 569 non-null float64\n",
            "smoothness error           569 non-null float64\n",
            "compactness error          569 non-null float64\n",
            "concavity error            569 non-null float64\n",
            "concave points error       569 non-null float64\n",
            "symmetry error             569 non-null float64\n",
            "fractal dimension error    569 non-null float64\n",
            "worst radius               569 non-null float64\n",
            "worst texture              569 non-null float64\n",
            "worst perimeter            569 non-null float64\n",
            "worst area                 569 non-null float64\n",
            "worst smoothness           569 non-null float64\n",
            "worst compactness          569 non-null float64\n",
            "worst concavity            569 non-null float64\n",
            "worst concave points       569 non-null float64\n",
            "worst symmetry             569 non-null float64\n",
            "worst fractal dimension    569 non-null float64\n",
            "benign_0__mal_1            569 non-null int64\n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 137.9 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_C88QbDd0ew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6703783f-8365-432d-b849-c9e082850d0a"
      },
      "source": [
        "df.describe().transpose()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean radius</th>\n",
              "      <td>569.0</td>\n",
              "      <td>14.127292</td>\n",
              "      <td>3.524049</td>\n",
              "      <td>6.981000</td>\n",
              "      <td>11.700000</td>\n",
              "      <td>13.370000</td>\n",
              "      <td>15.780000</td>\n",
              "      <td>28.11000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean texture</th>\n",
              "      <td>569.0</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>39.28000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean perimeter</th>\n",
              "      <td>569.0</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>188.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean area</th>\n",
              "      <td>569.0</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>2501.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean smoothness</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.16340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean compactness</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.34540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concavity</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.42680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concave points</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.20120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean symmetry</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.30400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.062798</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.049960</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>0.09744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.405172</td>\n",
              "      <td>0.277313</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.324200</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>2.87300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>1.216853</td>\n",
              "      <td>0.551648</td>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.833900</td>\n",
              "      <td>1.108000</td>\n",
              "      <td>1.474000</td>\n",
              "      <td>4.88500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>2.866059</td>\n",
              "      <td>2.021855</td>\n",
              "      <td>0.757000</td>\n",
              "      <td>1.606000</td>\n",
              "      <td>2.287000</td>\n",
              "      <td>3.357000</td>\n",
              "      <td>21.98000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>40.337079</td>\n",
              "      <td>45.491006</td>\n",
              "      <td>6.802000</td>\n",
              "      <td>17.850000</td>\n",
              "      <td>24.530000</td>\n",
              "      <td>45.190000</td>\n",
              "      <td>542.20000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.005169</td>\n",
              "      <td>0.006380</td>\n",
              "      <td>0.008146</td>\n",
              "      <td>0.03113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.020450</td>\n",
              "      <td>0.032450</td>\n",
              "      <td>0.13540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.030186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>0.042050</td>\n",
              "      <td>0.39600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.011796</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007638</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.05279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.015160</td>\n",
              "      <td>0.018730</td>\n",
              "      <td>0.023480</td>\n",
              "      <td>0.07895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal dimension error</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.003795</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>0.02984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst radius</th>\n",
              "      <td>569.0</td>\n",
              "      <td>16.269190</td>\n",
              "      <td>4.833242</td>\n",
              "      <td>7.930000</td>\n",
              "      <td>13.010000</td>\n",
              "      <td>14.970000</td>\n",
              "      <td>18.790000</td>\n",
              "      <td>36.04000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst texture</th>\n",
              "      <td>569.0</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>49.54000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst perimeter</th>\n",
              "      <td>569.0</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>251.20000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst area</th>\n",
              "      <td>569.0</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>4254.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst smoothness</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.22260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst compactness</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>1.05800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst concavity</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>1.25200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst concave points</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.29100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst symmetry</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.66380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.083946</td>\n",
              "      <td>0.018061</td>\n",
              "      <td>0.055040</td>\n",
              "      <td>0.071460</td>\n",
              "      <td>0.080040</td>\n",
              "      <td>0.092080</td>\n",
              "      <td>0.20750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>benign_0__mal_1</th>\n",
              "      <td>569.0</td>\n",
              "      <td>0.627417</td>\n",
              "      <td>0.483918</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         count        mean  ...          75%         max\n",
              "mean radius              569.0   14.127292  ...    15.780000    28.11000\n",
              "mean texture             569.0   19.289649  ...    21.800000    39.28000\n",
              "mean perimeter           569.0   91.969033  ...   104.100000   188.50000\n",
              "mean area                569.0  654.889104  ...   782.700000  2501.00000\n",
              "mean smoothness          569.0    0.096360  ...     0.105300     0.16340\n",
              "mean compactness         569.0    0.104341  ...     0.130400     0.34540\n",
              "mean concavity           569.0    0.088799  ...     0.130700     0.42680\n",
              "mean concave points      569.0    0.048919  ...     0.074000     0.20120\n",
              "mean symmetry            569.0    0.181162  ...     0.195700     0.30400\n",
              "mean fractal dimension   569.0    0.062798  ...     0.066120     0.09744\n",
              "radius error             569.0    0.405172  ...     0.478900     2.87300\n",
              "texture error            569.0    1.216853  ...     1.474000     4.88500\n",
              "perimeter error          569.0    2.866059  ...     3.357000    21.98000\n",
              "area error               569.0   40.337079  ...    45.190000   542.20000\n",
              "smoothness error         569.0    0.007041  ...     0.008146     0.03113\n",
              "compactness error        569.0    0.025478  ...     0.032450     0.13540\n",
              "concavity error          569.0    0.031894  ...     0.042050     0.39600\n",
              "concave points error     569.0    0.011796  ...     0.014710     0.05279\n",
              "symmetry error           569.0    0.020542  ...     0.023480     0.07895\n",
              "fractal dimension error  569.0    0.003795  ...     0.004558     0.02984\n",
              "worst radius             569.0   16.269190  ...    18.790000    36.04000\n",
              "worst texture            569.0   25.677223  ...    29.720000    49.54000\n",
              "worst perimeter          569.0  107.261213  ...   125.400000   251.20000\n",
              "worst area               569.0  880.583128  ...  1084.000000  4254.00000\n",
              "worst smoothness         569.0    0.132369  ...     0.146000     0.22260\n",
              "worst compactness        569.0    0.254265  ...     0.339100     1.05800\n",
              "worst concavity          569.0    0.272188  ...     0.382900     1.25200\n",
              "worst concave points     569.0    0.114606  ...     0.161400     0.29100\n",
              "worst symmetry           569.0    0.290076  ...     0.317900     0.66380\n",
              "worst fractal dimension  569.0    0.083946  ...     0.092080     0.20750\n",
              "benign_0__mal_1          569.0    0.627417  ...     1.000000     1.00000\n",
              "\n",
              "[31 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZddUuXV3d6QB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "a1fd90e0-ff59-4f0e-b2dd-94bf6938045c"
      },
      "source": [
        "df.corr()['benign_0__mal_1'].sort_values(ascending=False)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "benign_0__mal_1            1.000000\n",
              "smoothness error           0.067016\n",
              "mean fractal dimension     0.012838\n",
              "texture error              0.008303\n",
              "symmetry error             0.006522\n",
              "fractal dimension error   -0.077972\n",
              "concavity error           -0.253730\n",
              "compactness error         -0.292999\n",
              "worst fractal dimension   -0.323872\n",
              "mean symmetry             -0.330499\n",
              "mean smoothness           -0.358560\n",
              "concave points error      -0.408042\n",
              "mean texture              -0.415185\n",
              "worst symmetry            -0.416294\n",
              "worst smoothness          -0.421465\n",
              "worst texture             -0.456903\n",
              "area error                -0.548236\n",
              "perimeter error           -0.556141\n",
              "radius error              -0.567134\n",
              "worst compactness         -0.590998\n",
              "mean compactness          -0.596534\n",
              "worst concavity           -0.659610\n",
              "mean concavity            -0.696360\n",
              "mean area                 -0.708984\n",
              "mean radius               -0.730029\n",
              "worst area                -0.733825\n",
              "mean perimeter            -0.742636\n",
              "worst radius              -0.776454\n",
              "mean concave points       -0.776614\n",
              "worst perimeter           -0.782914\n",
              "worst concave points      -0.793566\n",
              "Name: benign_0__mal_1, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krTIiau6eFR7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "86560ac4-21c5-4039-85de-1d14591cd9ea"
      },
      "source": [
        "sns.countplot(x='benign_0__mal_1', data = df)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f80e4df5b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEHCAYAAABBW1qbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1HihWZLUGAqSpMZQkCQ1hoIkqfn/2zvNOui6DpoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CPsad6Oecph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_dropper(df) :\n",
        "  corr_df = df.corr()['benign_0__mal_1'] #choose target label here\n",
        "  names = list(corr_df.index)\n",
        "  cols_to_drop = list()\n",
        "  cols = df.shape[1]\n",
        "  for i in range(0, cols) :\n",
        "    cor = corr_df[i]\n",
        "    if abs(cor) < 0.01 : #change value to increase sensitivity\n",
        "      cols_to_drop.append(names[i])\n",
        "  return df.drop(labels=cols_to_drop, axis=1 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukN8U9OIfRbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a241e383-ea11-4e9c-fefc-33fd2978feb5"
      },
      "source": [
        "clean_df = feature_dropper(df)\n",
        "print(clean_df.head())\n",
        "print(clean_df.columns)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   mean radius  mean texture  ...  worst fractal dimension  benign_0__mal_1\n",
            "0        17.99         10.38  ...                  0.11890                0\n",
            "1        20.57         17.77  ...                  0.08902                0\n",
            "2        19.69         21.25  ...                  0.08758                0\n",
            "3        11.42         20.38  ...                  0.17300                0\n",
            "4        20.29         14.34  ...                  0.07678                0\n",
            "\n",
            "[5 rows x 29 columns]\n",
            "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
            "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
            "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
            "       'radius error', 'perimeter error', 'area error', 'smoothness error',\n",
            "       'compactness error', 'concavity error', 'concave points error',\n",
            "       'fractal dimension error', 'worst radius', 'worst texture',\n",
            "       'worst perimeter', 'worst area', 'worst smoothness',\n",
            "       'worst compactness', 'worst concavity', 'worst concave points',\n",
            "       'worst symmetry', 'worst fractal dimension', 'benign_0__mal_1'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayx7ajH-hIKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = clean_df.drop('benign_0__mal_1', axis = 1).values\n",
        "y = clean_df['benign_0__mal_1'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfNMP4jUkr7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import  train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIzY3-u0lGww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWH2NYyDleh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5-7FI7Gl1S2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7f7f486-9c1b-4501-ec96-194d29f8527e"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(426, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNs-DyTRmBsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units=30, activation='relu'))\n",
        "model.add(Dense(units=15, activation='relu'))\n",
        "\n",
        "#Binary Classification\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn3MhL5WmrUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbf79d54-9af4-4101-a7c3-db6f298f6253"
      },
      "source": [
        "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test,y_test), batch_size=64)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 426 samples, validate on 143 samples\n",
            "Epoch 1/600\n",
            "426/426 [==============================] - 0s 907us/sample - loss: 0.6983 - val_loss: 0.6779\n",
            "Epoch 2/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.6874 - val_loss: 0.6734\n",
            "Epoch 3/600\n",
            "426/426 [==============================] - 0s 46us/sample - loss: 0.6798 - val_loss: 0.6673\n",
            "Epoch 4/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.6710 - val_loss: 0.6566\n",
            "Epoch 5/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.6589 - val_loss: 0.6410\n",
            "Epoch 6/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.6463 - val_loss: 0.6251\n",
            "Epoch 7/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.6288 - val_loss: 0.6057\n",
            "Epoch 8/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.6102 - val_loss: 0.5844\n",
            "Epoch 9/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.5904 - val_loss: 0.5613\n",
            "Epoch 10/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.5685 - val_loss: 0.5371\n",
            "Epoch 11/600\n",
            "426/426 [==============================] - 0s 83us/sample - loss: 0.5443 - val_loss: 0.5117\n",
            "Epoch 12/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.5171 - val_loss: 0.4782\n",
            "Epoch 13/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.4841 - val_loss: 0.4446\n",
            "Epoch 14/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.4518 - val_loss: 0.4093\n",
            "Epoch 15/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.4184 - val_loss: 0.3788\n",
            "Epoch 16/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.3868 - val_loss: 0.3456\n",
            "Epoch 17/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.3577 - val_loss: 0.3235\n",
            "Epoch 18/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.3314 - val_loss: 0.2886\n",
            "Epoch 19/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.3062 - val_loss: 0.2715\n",
            "Epoch 20/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.2847 - val_loss: 0.2524\n",
            "Epoch 21/600\n",
            "426/426 [==============================] - 0s 70us/sample - loss: 0.2655 - val_loss: 0.2317\n",
            "Epoch 22/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.2488 - val_loss: 0.2117\n",
            "Epoch 23/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.2341 - val_loss: 0.2029\n",
            "Epoch 24/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.2219 - val_loss: 0.1950\n",
            "Epoch 25/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.2121 - val_loss: 0.1751\n",
            "Epoch 26/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.2012 - val_loss: 0.1748\n",
            "Epoch 27/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.1937 - val_loss: 0.1573\n",
            "Epoch 28/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.1841 - val_loss: 0.1583\n",
            "Epoch 29/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.1769 - val_loss: 0.1489\n",
            "Epoch 30/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.1710 - val_loss: 0.1400\n",
            "Epoch 31/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.1643 - val_loss: 0.1348\n",
            "Epoch 32/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.1589 - val_loss: 0.1349\n",
            "Epoch 33/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.1542 - val_loss: 0.1273\n",
            "Epoch 34/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.1498 - val_loss: 0.1193\n",
            "Epoch 35/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.1447 - val_loss: 0.1175\n",
            "Epoch 36/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.1404 - val_loss: 0.1149\n",
            "Epoch 37/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.1365 - val_loss: 0.1085\n",
            "Epoch 38/600\n",
            "426/426 [==============================] - 0s 79us/sample - loss: 0.1327 - val_loss: 0.1076\n",
            "Epoch 39/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.1288 - val_loss: 0.1036\n",
            "Epoch 40/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.1253 - val_loss: 0.0999\n",
            "Epoch 41/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.1223 - val_loss: 0.0973\n",
            "Epoch 42/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.1191 - val_loss: 0.0954\n",
            "Epoch 43/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.1162 - val_loss: 0.0902\n",
            "Epoch 44/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.1131 - val_loss: 0.0898\n",
            "Epoch 45/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.1120 - val_loss: 0.0911\n",
            "Epoch 46/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.1083 - val_loss: 0.0813\n",
            "Epoch 47/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.1063 - val_loss: 0.0830\n",
            "Epoch 48/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.1041 - val_loss: 0.0857\n",
            "Epoch 49/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.1011 - val_loss: 0.0764\n",
            "Epoch 50/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.1007 - val_loss: 0.0781\n",
            "Epoch 51/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0972 - val_loss: 0.0761\n",
            "Epoch 52/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0950 - val_loss: 0.0724\n",
            "Epoch 53/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0937 - val_loss: 0.0724\n",
            "Epoch 54/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0924 - val_loss: 0.0721\n",
            "Epoch 55/600\n",
            "426/426 [==============================] - 0s 45us/sample - loss: 0.0897 - val_loss: 0.0734\n",
            "Epoch 56/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0887 - val_loss: 0.0677\n",
            "Epoch 57/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0884 - val_loss: 0.0705\n",
            "Epoch 58/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0859 - val_loss: 0.0640\n",
            "Epoch 59/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0852 - val_loss: 0.0684\n",
            "Epoch 60/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0847 - val_loss: 0.0630\n",
            "Epoch 61/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0828 - val_loss: 0.0687\n",
            "Epoch 62/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0804 - val_loss: 0.0612\n",
            "Epoch 63/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0793 - val_loss: 0.0623\n",
            "Epoch 64/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0792 - val_loss: 0.0614\n",
            "Epoch 65/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0804 - val_loss: 0.0678\n",
            "Epoch 66/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0779 - val_loss: 0.0550\n",
            "Epoch 67/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0767 - val_loss: 0.0614\n",
            "Epoch 68/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0752 - val_loss: 0.0627\n",
            "Epoch 69/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0738 - val_loss: 0.0561\n",
            "Epoch 70/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0726 - val_loss: 0.0579\n",
            "Epoch 71/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0731 - val_loss: 0.0548\n",
            "Epoch 72/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0711 - val_loss: 0.0575\n",
            "Epoch 73/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0702 - val_loss: 0.0591\n",
            "Epoch 74/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0700 - val_loss: 0.0554\n",
            "Epoch 75/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0691 - val_loss: 0.0559\n",
            "Epoch 76/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0682 - val_loss: 0.0513\n",
            "Epoch 77/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0681 - val_loss: 0.0527\n",
            "Epoch 78/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0673 - val_loss: 0.0577\n",
            "Epoch 79/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0665 - val_loss: 0.0526\n",
            "Epoch 80/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0661 - val_loss: 0.0508\n",
            "Epoch 81/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0649 - val_loss: 0.0553\n",
            "Epoch 82/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0649 - val_loss: 0.0516\n",
            "Epoch 83/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0657 - val_loss: 0.0498\n",
            "Epoch 84/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0640 - val_loss: 0.0555\n",
            "Epoch 85/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0632 - val_loss: 0.0491\n",
            "Epoch 86/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0631 - val_loss: 0.0480\n",
            "Epoch 87/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0642 - val_loss: 0.0566\n",
            "Epoch 88/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0620 - val_loss: 0.0462\n",
            "Epoch 89/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.0616 - val_loss: 0.0489\n",
            "Epoch 90/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0605 - val_loss: 0.0516\n",
            "Epoch 91/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0607 - val_loss: 0.0529\n",
            "Epoch 92/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0608 - val_loss: 0.0487\n",
            "Epoch 93/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0600 - val_loss: 0.0446\n",
            "Epoch 94/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0599 - val_loss: 0.0518\n",
            "Epoch 95/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0591 - val_loss: 0.0512\n",
            "Epoch 96/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0586 - val_loss: 0.0479\n",
            "Epoch 97/600\n",
            "426/426 [==============================] - 0s 78us/sample - loss: 0.0590 - val_loss: 0.0470\n",
            "Epoch 98/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0574 - val_loss: 0.0512\n",
            "Epoch 99/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0577 - val_loss: 0.0487\n",
            "Epoch 100/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0572 - val_loss: 0.0485\n",
            "Epoch 101/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0583 - val_loss: 0.0446\n",
            "Epoch 102/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0575 - val_loss: 0.0554\n",
            "Epoch 103/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0584 - val_loss: 0.0483\n",
            "Epoch 104/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0598 - val_loss: 0.0412\n",
            "Epoch 105/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0564 - val_loss: 0.0565\n",
            "Epoch 106/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0582 - val_loss: 0.0464\n",
            "Epoch 107/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0554 - val_loss: 0.0483\n",
            "Epoch 108/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0550 - val_loss: 0.0447\n",
            "Epoch 109/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0546 - val_loss: 0.0468\n",
            "Epoch 110/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0545 - val_loss: 0.0471\n",
            "Epoch 111/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0551 - val_loss: 0.0426\n",
            "Epoch 112/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0549 - val_loss: 0.0501\n",
            "Epoch 113/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0539 - val_loss: 0.0464\n",
            "Epoch 114/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0541 - val_loss: 0.0453\n",
            "Epoch 115/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0530 - val_loss: 0.0449\n",
            "Epoch 116/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0530 - val_loss: 0.0448\n",
            "Epoch 117/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0536 - val_loss: 0.0424\n",
            "Epoch 118/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0522 - val_loss: 0.0511\n",
            "Epoch 119/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0531 - val_loss: 0.0464\n",
            "Epoch 120/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0519 - val_loss: 0.0404\n",
            "Epoch 121/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0532 - val_loss: 0.0459\n",
            "Epoch 122/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0519 - val_loss: 0.0428\n",
            "Epoch 123/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0514 - val_loss: 0.0442\n",
            "Epoch 124/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0561 - val_loss: 0.0531\n",
            "Epoch 125/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0504 - val_loss: 0.0388\n",
            "Epoch 126/600\n",
            "426/426 [==============================] - 0s 47us/sample - loss: 0.0519 - val_loss: 0.0422\n",
            "Epoch 127/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0509 - val_loss: 0.0477\n",
            "Epoch 128/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0507 - val_loss: 0.0420\n",
            "Epoch 129/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 130/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0497 - val_loss: 0.0448\n",
            "Epoch 131/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0494 - val_loss: 0.0428\n",
            "Epoch 132/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0494 - val_loss: 0.0449\n",
            "Epoch 133/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0491 - val_loss: 0.0439\n",
            "Epoch 134/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0516 - val_loss: 0.0402\n",
            "Epoch 135/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0493 - val_loss: 0.0531\n",
            "Epoch 136/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0496 - val_loss: 0.0432\n",
            "Epoch 137/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0495 - val_loss: 0.0406\n",
            "Epoch 138/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0479 - val_loss: 0.0477\n",
            "Epoch 139/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0507 - val_loss: 0.0534\n",
            "Epoch 140/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0480 - val_loss: 0.0379\n",
            "Epoch 141/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0492 - val_loss: 0.0416\n",
            "Epoch 142/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0472 - val_loss: 0.0494\n",
            "Epoch 143/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0480 - val_loss: 0.0458\n",
            "Epoch 144/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0482 - val_loss: 0.0380\n",
            "Epoch 145/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0476 - val_loss: 0.0458\n",
            "Epoch 146/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0475 - val_loss: 0.0495\n",
            "Epoch 147/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0469 - val_loss: 0.0435\n",
            "Epoch 148/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0464 - val_loss: 0.0406\n",
            "Epoch 149/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0468 - val_loss: 0.0442\n",
            "Epoch 150/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0468 - val_loss: 0.0427\n",
            "Epoch 151/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0472 - val_loss: 0.0476\n",
            "Epoch 152/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0459 - val_loss: 0.0403\n",
            "Epoch 153/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0460 - val_loss: 0.0440\n",
            "Epoch 154/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0465 - val_loss: 0.0476\n",
            "Epoch 155/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0452 - val_loss: 0.0395\n",
            "Epoch 156/600\n",
            "426/426 [==============================] - 0s 46us/sample - loss: 0.0463 - val_loss: 0.0430\n",
            "Epoch 157/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0452 - val_loss: 0.0512\n",
            "Epoch 158/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0458 - val_loss: 0.0419\n",
            "Epoch 159/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0455 - val_loss: 0.0419\n",
            "Epoch 160/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0446 - val_loss: 0.0484\n",
            "Epoch 161/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0455 - val_loss: 0.0483\n",
            "Epoch 162/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0452 - val_loss: 0.0393\n",
            "Epoch 163/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0451 - val_loss: 0.0445\n",
            "Epoch 164/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.0450 - val_loss: 0.0515\n",
            "Epoch 165/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0471 - val_loss: 0.0392\n",
            "Epoch 166/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0454 - val_loss: 0.0524\n",
            "Epoch 167/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0441 - val_loss: 0.0457\n",
            "Epoch 168/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0431 - val_loss: 0.0394\n",
            "Epoch 169/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0440 - val_loss: 0.0423\n",
            "Epoch 170/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0447 - val_loss: 0.0518\n",
            "Epoch 171/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0431 - val_loss: 0.0408\n",
            "Epoch 172/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0430 - val_loss: 0.0433\n",
            "Epoch 173/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0443 - val_loss: 0.0525\n",
            "Epoch 174/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0434 - val_loss: 0.0405\n",
            "Epoch 175/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0429 - val_loss: 0.0439\n",
            "Epoch 176/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0421 - val_loss: 0.0466\n",
            "Epoch 177/600\n",
            "426/426 [==============================] - 0s 46us/sample - loss: 0.0421 - val_loss: 0.0484\n",
            "Epoch 178/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0421 - val_loss: 0.0442\n",
            "Epoch 179/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0439 - val_loss: 0.0402\n",
            "Epoch 180/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0487 - val_loss: 0.0679\n",
            "Epoch 181/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0421 - val_loss: 0.0349\n",
            "Epoch 182/600\n",
            "426/426 [==============================] - 0s 46us/sample - loss: 0.0452 - val_loss: 0.0405\n",
            "Epoch 183/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0417 - val_loss: 0.0585\n",
            "Epoch 184/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0424 - val_loss: 0.0429\n",
            "Epoch 185/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0421 - val_loss: 0.0430\n",
            "Epoch 186/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0408 - val_loss: 0.0455\n",
            "Epoch 187/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0409 - val_loss: 0.0489\n",
            "Epoch 188/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0413 - val_loss: 0.0420\n",
            "Epoch 189/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0406 - val_loss: 0.0495\n",
            "Epoch 190/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0410 - val_loss: 0.0476\n",
            "Epoch 191/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0417 - val_loss: 0.0413\n",
            "Epoch 192/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0406 - val_loss: 0.0498\n",
            "Epoch 193/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0399 - val_loss: 0.0455\n",
            "Epoch 194/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0399 - val_loss: 0.0420\n",
            "Epoch 195/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0403 - val_loss: 0.0446\n",
            "Epoch 196/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0397 - val_loss: 0.0530\n",
            "Epoch 197/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0407 - val_loss: 0.0448\n",
            "Epoch 198/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0396 - val_loss: 0.0498\n",
            "Epoch 199/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0394 - val_loss: 0.0422\n",
            "Epoch 200/600\n",
            "426/426 [==============================] - 0s 47us/sample - loss: 0.0393 - val_loss: 0.0443\n",
            "Epoch 201/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0395 - val_loss: 0.0493\n",
            "Epoch 202/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0389 - val_loss: 0.0439\n",
            "Epoch 203/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0385 - val_loss: 0.0492\n",
            "Epoch 204/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0386 - val_loss: 0.0488\n",
            "Epoch 205/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0392 - val_loss: 0.0436\n",
            "Epoch 206/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0387 - val_loss: 0.0495\n",
            "Epoch 207/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0380 - val_loss: 0.0460\n",
            "Epoch 208/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0393 - val_loss: 0.0445\n",
            "Epoch 209/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0379 - val_loss: 0.0586\n",
            "Epoch 210/600\n",
            "426/426 [==============================] - 0s 76us/sample - loss: 0.0388 - val_loss: 0.0453\n",
            "Epoch 211/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0387 - val_loss: 0.0493\n",
            "Epoch 212/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0403 - val_loss: 0.0426\n",
            "Epoch 213/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0377 - val_loss: 0.0576\n",
            "Epoch 214/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0385 - val_loss: 0.0449\n",
            "Epoch 215/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0373 - val_loss: 0.0473\n",
            "Epoch 216/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0370 - val_loss: 0.0499\n",
            "Epoch 217/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0369 - val_loss: 0.0480\n",
            "Epoch 218/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0377 - val_loss: 0.0455\n",
            "Epoch 219/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0376 - val_loss: 0.0509\n",
            "Epoch 220/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0374 - val_loss: 0.0403\n",
            "Epoch 221/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0369 - val_loss: 0.0561\n",
            "Epoch 222/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0367 - val_loss: 0.0478\n",
            "Epoch 223/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0365 - val_loss: 0.0453\n",
            "Epoch 224/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0363 - val_loss: 0.0462\n",
            "Epoch 225/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0366 - val_loss: 0.0531\n",
            "Epoch 226/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0356 - val_loss: 0.0448\n",
            "Epoch 227/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0359 - val_loss: 0.0440\n",
            "Epoch 228/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0358 - val_loss: 0.0519\n",
            "Epoch 229/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0368 - val_loss: 0.0468\n",
            "Epoch 230/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0405 - val_loss: 0.0668\n",
            "Epoch 231/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0384 - val_loss: 0.0360\n",
            "Epoch 232/600\n",
            "426/426 [==============================] - 0s 45us/sample - loss: 0.0384 - val_loss: 0.0514\n",
            "Epoch 233/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0365 - val_loss: 0.0496\n",
            "Epoch 234/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0350 - val_loss: 0.0552\n",
            "Epoch 235/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0359 - val_loss: 0.0533\n",
            "Epoch 236/600\n",
            "426/426 [==============================] - 0s 70us/sample - loss: 0.0373 - val_loss: 0.0387\n",
            "Epoch 237/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0339 - val_loss: 0.0547\n",
            "Epoch 238/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0366 - val_loss: 0.0583\n",
            "Epoch 239/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0355 - val_loss: 0.0397\n",
            "Epoch 240/600\n",
            "426/426 [==============================] - 0s 73us/sample - loss: 0.0354 - val_loss: 0.0536\n",
            "Epoch 241/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0345 - val_loss: 0.0567\n",
            "Epoch 242/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0346 - val_loss: 0.0461\n",
            "Epoch 243/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0344 - val_loss: 0.0478\n",
            "Epoch 244/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0337 - val_loss: 0.0516\n",
            "Epoch 245/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0338 - val_loss: 0.0528\n",
            "Epoch 246/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0335 - val_loss: 0.0497\n",
            "Epoch 247/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0341 - val_loss: 0.0465\n",
            "Epoch 248/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0335 - val_loss: 0.0532\n",
            "Epoch 249/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0342 - val_loss: 0.0582\n",
            "Epoch 250/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0332 - val_loss: 0.0479\n",
            "Epoch 251/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0332 - val_loss: 0.0530\n",
            "Epoch 252/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0329 - val_loss: 0.0528\n",
            "Epoch 253/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0327 - val_loss: 0.0505\n",
            "Epoch 254/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0326 - val_loss: 0.0510\n",
            "Epoch 255/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0330 - val_loss: 0.0504\n",
            "Epoch 256/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0332 - val_loss: 0.0545\n",
            "Epoch 257/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0327 - val_loss: 0.0502\n",
            "Epoch 258/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0323 - val_loss: 0.0585\n",
            "Epoch 259/600\n",
            "426/426 [==============================] - 0s 84us/sample - loss: 0.0324 - val_loss: 0.0509\n",
            "Epoch 260/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0320 - val_loss: 0.0525\n",
            "Epoch 261/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0319 - val_loss: 0.0514\n",
            "Epoch 262/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0325 - val_loss: 0.0530\n",
            "Epoch 263/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0318 - val_loss: 0.0597\n",
            "Epoch 264/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0316 - val_loss: 0.0493\n",
            "Epoch 265/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0322 - val_loss: 0.0493\n",
            "Epoch 266/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0312 - val_loss: 0.0573\n",
            "Epoch 267/600\n",
            "426/426 [==============================] - 0s 47us/sample - loss: 0.0315 - val_loss: 0.0546\n",
            "Epoch 268/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0322 - val_loss: 0.0520\n",
            "Epoch 269/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0334 - val_loss: 0.0474\n",
            "Epoch 270/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0311 - val_loss: 0.0667\n",
            "Epoch 271/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0332 - val_loss: 0.0539\n",
            "Epoch 272/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0308 - val_loss: 0.0537\n",
            "Epoch 273/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0310 - val_loss: 0.0527\n",
            "Epoch 274/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0307 - val_loss: 0.0594\n",
            "Epoch 275/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0312 - val_loss: 0.0583\n",
            "Epoch 276/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0310 - val_loss: 0.0439\n",
            "Epoch 277/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0318 - val_loss: 0.0642\n",
            "Epoch 278/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0314 - val_loss: 0.0480\n",
            "Epoch 279/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0307 - val_loss: 0.0548\n",
            "Epoch 280/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0303 - val_loss: 0.0638\n",
            "Epoch 281/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0327 - val_loss: 0.0491\n",
            "Epoch 282/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0297 - val_loss: 0.0576\n",
            "Epoch 283/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0303 - val_loss: 0.0666\n",
            "Epoch 284/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0297 - val_loss: 0.0512\n",
            "Epoch 285/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0313 - val_loss: 0.0513\n",
            "Epoch 286/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0308 - val_loss: 0.0648\n",
            "Epoch 287/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0306 - val_loss: 0.0468\n",
            "Epoch 288/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.0293 - val_loss: 0.0599\n",
            "Epoch 289/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0295 - val_loss: 0.0615\n",
            "Epoch 290/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0292 - val_loss: 0.0581\n",
            "Epoch 291/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0295 - val_loss: 0.0516\n",
            "Epoch 292/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0286 - val_loss: 0.0598\n",
            "Epoch 293/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0297 - val_loss: 0.0625\n",
            "Epoch 294/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0294 - val_loss: 0.0472\n",
            "Epoch 295/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0287 - val_loss: 0.0580\n",
            "Epoch 296/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0285 - val_loss: 0.0612\n",
            "Epoch 297/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0300 - val_loss: 0.0553\n",
            "Epoch 298/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0282 - val_loss: 0.0605\n",
            "Epoch 299/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0286 - val_loss: 0.0536\n",
            "Epoch 300/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0283 - val_loss: 0.0564\n",
            "Epoch 301/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0289 - val_loss: 0.0558\n",
            "Epoch 302/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0275 - val_loss: 0.0661\n",
            "Epoch 303/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.0286 - val_loss: 0.0515\n",
            "Epoch 304/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0277 - val_loss: 0.0567\n",
            "Epoch 305/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0274 - val_loss: 0.0574\n",
            "Epoch 306/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0276 - val_loss: 0.0539\n",
            "Epoch 307/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0278 - val_loss: 0.0614\n",
            "Epoch 308/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0273 - val_loss: 0.0554\n",
            "Epoch 309/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0272 - val_loss: 0.0547\n",
            "Epoch 310/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0273 - val_loss: 0.0524\n",
            "Epoch 311/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0278 - val_loss: 0.0617\n",
            "Epoch 312/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0266 - val_loss: 0.0519\n",
            "Epoch 313/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0269 - val_loss: 0.0577\n",
            "Epoch 314/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0268 - val_loss: 0.0566\n",
            "Epoch 315/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0283 - val_loss: 0.0559\n",
            "Epoch 316/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0263 - val_loss: 0.0633\n",
            "Epoch 317/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0265 - val_loss: 0.0587\n",
            "Epoch 318/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0269 - val_loss: 0.0670\n",
            "Epoch 319/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0263 - val_loss: 0.0567\n",
            "Epoch 320/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0262 - val_loss: 0.0573\n",
            "Epoch 321/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0259 - val_loss: 0.0561\n",
            "Epoch 322/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0258 - val_loss: 0.0636\n",
            "Epoch 323/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0281 - val_loss: 0.0556\n",
            "Epoch 324/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0263 - val_loss: 0.0650\n",
            "Epoch 325/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0256 - val_loss: 0.0543\n",
            "Epoch 326/600\n",
            "426/426 [==============================] - 0s 74us/sample - loss: 0.0263 - val_loss: 0.0598\n",
            "Epoch 327/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0267 - val_loss: 0.0578\n",
            "Epoch 328/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0255 - val_loss: 0.0527\n",
            "Epoch 329/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0279 - val_loss: 0.0682\n",
            "Epoch 330/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0257 - val_loss: 0.0479\n",
            "Epoch 331/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0264 - val_loss: 0.0671\n",
            "Epoch 332/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0251 - val_loss: 0.0574\n",
            "Epoch 333/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0283 - val_loss: 0.0505\n",
            "Epoch 334/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0342 - val_loss: 0.0899\n",
            "Epoch 335/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0312 - val_loss: 0.0393\n",
            "Epoch 336/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0284 - val_loss: 0.0710\n",
            "Epoch 337/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0266 - val_loss: 0.0658\n",
            "Epoch 338/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0255 - val_loss: 0.0505\n",
            "Epoch 339/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0252 - val_loss: 0.0705\n",
            "Epoch 340/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0258 - val_loss: 0.0577\n",
            "Epoch 341/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0275 - val_loss: 0.0668\n",
            "Epoch 342/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0246 - val_loss: 0.0533\n",
            "Epoch 343/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0245 - val_loss: 0.0646\n",
            "Epoch 344/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0242 - val_loss: 0.0633\n",
            "Epoch 345/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0254 - val_loss: 0.0568\n",
            "Epoch 346/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0246 - val_loss: 0.0693\n",
            "Epoch 347/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0233 - val_loss: 0.0570\n",
            "Epoch 348/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0244 - val_loss: 0.0588\n",
            "Epoch 349/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0261 - val_loss: 0.0659\n",
            "Epoch 350/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0235 - val_loss: 0.0553\n",
            "Epoch 351/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0237 - val_loss: 0.0603\n",
            "Epoch 352/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0231 - val_loss: 0.0634\n",
            "Epoch 353/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0233 - val_loss: 0.0673\n",
            "Epoch 354/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0233 - val_loss: 0.0559\n",
            "Epoch 355/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0237 - val_loss: 0.0595\n",
            "Epoch 356/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0240 - val_loss: 0.0652\n",
            "Epoch 357/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0239 - val_loss: 0.0569\n",
            "Epoch 358/600\n",
            "426/426 [==============================] - 0s 45us/sample - loss: 0.0223 - val_loss: 0.0696\n",
            "Epoch 359/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0237 - val_loss: 0.0617\n",
            "Epoch 360/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0232 - val_loss: 0.0693\n",
            "Epoch 361/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0220 - val_loss: 0.0530\n",
            "Epoch 362/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0251 - val_loss: 0.0687\n",
            "Epoch 363/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0241 - val_loss: 0.0688\n",
            "Epoch 364/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0230 - val_loss: 0.0518\n",
            "Epoch 365/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0219 - val_loss: 0.0697\n",
            "Epoch 366/600\n",
            "426/426 [==============================] - 0s 47us/sample - loss: 0.0227 - val_loss: 0.0734\n",
            "Epoch 367/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0225 - val_loss: 0.0532\n",
            "Epoch 368/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0235 - val_loss: 0.0722\n",
            "Epoch 369/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0234 - val_loss: 0.0522\n",
            "Epoch 370/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0248 - val_loss: 0.0543\n",
            "Epoch 371/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0273 - val_loss: 0.0866\n",
            "Epoch 372/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0229 - val_loss: 0.0505\n",
            "Epoch 373/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0223 - val_loss: 0.0751\n",
            "Epoch 374/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0224 - val_loss: 0.0725\n",
            "Epoch 375/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0226 - val_loss: 0.0631\n",
            "Epoch 376/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0218 - val_loss: 0.0691\n",
            "Epoch 377/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0213 - val_loss: 0.0601\n",
            "Epoch 378/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0213 - val_loss: 0.0703\n",
            "Epoch 379/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0217 - val_loss: 0.0630\n",
            "Epoch 380/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0219 - val_loss: 0.0711\n",
            "Epoch 381/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0224 - val_loss: 0.0562\n",
            "Epoch 382/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0206 - val_loss: 0.0703\n",
            "Epoch 383/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0208 - val_loss: 0.0646\n",
            "Epoch 384/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0204 - val_loss: 0.0639\n",
            "Epoch 385/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0204 - val_loss: 0.0699\n",
            "Epoch 386/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0206 - val_loss: 0.0656\n",
            "Epoch 387/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0203 - val_loss: 0.0652\n",
            "Epoch 388/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0224 - val_loss: 0.0700\n",
            "Epoch 389/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0207 - val_loss: 0.0590\n",
            "Epoch 390/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0202 - val_loss: 0.0678\n",
            "Epoch 391/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0204 - val_loss: 0.0656\n",
            "Epoch 392/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0200 - val_loss: 0.0724\n",
            "Epoch 393/600\n",
            "426/426 [==============================] - 0s 72us/sample - loss: 0.0195 - val_loss: 0.0650\n",
            "Epoch 394/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0201 - val_loss: 0.0649\n",
            "Epoch 395/600\n",
            "426/426 [==============================] - 0s 81us/sample - loss: 0.0192 - val_loss: 0.0735\n",
            "Epoch 396/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0196 - val_loss: 0.0646\n",
            "Epoch 397/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0199 - val_loss: 0.0703\n",
            "Epoch 398/600\n",
            "426/426 [==============================] - 0s 47us/sample - loss: 0.0195 - val_loss: 0.0660\n",
            "Epoch 399/600\n",
            "426/426 [==============================] - 0s 45us/sample - loss: 0.0207 - val_loss: 0.0695\n",
            "Epoch 400/600\n",
            "426/426 [==============================] - 0s 46us/sample - loss: 0.0188 - val_loss: 0.0575\n",
            "Epoch 401/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0192 - val_loss: 0.0726\n",
            "Epoch 402/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0191 - val_loss: 0.0665\n",
            "Epoch 403/600\n",
            "426/426 [==============================] - 0s 46us/sample - loss: 0.0188 - val_loss: 0.0650\n",
            "Epoch 404/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0186 - val_loss: 0.0667\n",
            "Epoch 405/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0186 - val_loss: 0.0687\n",
            "Epoch 406/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0185 - val_loss: 0.0704\n",
            "Epoch 407/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0184 - val_loss: 0.0678\n",
            "Epoch 408/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0183 - val_loss: 0.0671\n",
            "Epoch 409/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0189 - val_loss: 0.0664\n",
            "Epoch 410/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0181 - val_loss: 0.0737\n",
            "Epoch 411/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0180 - val_loss: 0.0678\n",
            "Epoch 412/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0200 - val_loss: 0.0658\n",
            "Epoch 413/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0180 - val_loss: 0.0758\n",
            "Epoch 414/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0194 - val_loss: 0.0663\n",
            "Epoch 415/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0204 - val_loss: 0.0728\n",
            "Epoch 416/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0178 - val_loss: 0.0617\n",
            "Epoch 417/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0186 - val_loss: 0.0763\n",
            "Epoch 418/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0177 - val_loss: 0.0721\n",
            "Epoch 419/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0176 - val_loss: 0.0720\n",
            "Epoch 420/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.0184 - val_loss: 0.0646\n",
            "Epoch 421/600\n",
            "426/426 [==============================] - 0s 72us/sample - loss: 0.0191 - val_loss: 0.0760\n",
            "Epoch 422/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0179 - val_loss: 0.0645\n",
            "Epoch 423/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0174 - val_loss: 0.0743\n",
            "Epoch 424/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0180 - val_loss: 0.0721\n",
            "Epoch 425/600\n",
            "426/426 [==============================] - 0s 71us/sample - loss: 0.0172 - val_loss: 0.0769\n",
            "Epoch 426/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0174 - val_loss: 0.0734\n",
            "Epoch 427/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0169 - val_loss: 0.0662\n",
            "Epoch 428/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0190 - val_loss: 0.0630\n",
            "Epoch 429/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0197 - val_loss: 0.0884\n",
            "Epoch 430/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0191 - val_loss: 0.0572\n",
            "Epoch 431/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0165 - val_loss: 0.0861\n",
            "Epoch 432/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0199 - val_loss: 0.0686\n",
            "Epoch 433/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0171 - val_loss: 0.0829\n",
            "Epoch 434/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0162 - val_loss: 0.0667\n",
            "Epoch 435/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0171 - val_loss: 0.0693\n",
            "Epoch 436/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0161 - val_loss: 0.0735\n",
            "Epoch 437/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0163 - val_loss: 0.0754\n",
            "Epoch 438/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0168 - val_loss: 0.0701\n",
            "Epoch 439/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0162 - val_loss: 0.0777\n",
            "Epoch 440/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0163 - val_loss: 0.0719\n",
            "Epoch 441/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0159 - val_loss: 0.0802\n",
            "Epoch 442/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0160 - val_loss: 0.0683\n",
            "Epoch 443/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0156 - val_loss: 0.0770\n",
            "Epoch 444/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0166 - val_loss: 0.0715\n",
            "Epoch 445/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0162 - val_loss: 0.0748\n",
            "Epoch 446/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0173 - val_loss: 0.0682\n",
            "Epoch 447/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0157 - val_loss: 0.0821\n",
            "Epoch 448/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0161 - val_loss: 0.0723\n",
            "Epoch 449/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0171 - val_loss: 0.0724\n",
            "Epoch 450/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0147 - val_loss: 0.0843\n",
            "Epoch 451/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0156 - val_loss: 0.0743\n",
            "Epoch 452/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0153 - val_loss: 0.0745\n",
            "Epoch 453/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0151 - val_loss: 0.0817\n",
            "Epoch 454/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0150 - val_loss: 0.0716\n",
            "Epoch 455/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0152 - val_loss: 0.0704\n",
            "Epoch 456/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0170 - val_loss: 0.0816\n",
            "Epoch 457/600\n",
            "426/426 [==============================] - 0s 45us/sample - loss: 0.0161 - val_loss: 0.0624\n",
            "Epoch 458/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0146 - val_loss: 0.0843\n",
            "Epoch 459/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0151 - val_loss: 0.0718\n",
            "Epoch 460/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0170 - val_loss: 0.0624\n",
            "Epoch 461/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.0156 - val_loss: 0.0994\n",
            "Epoch 462/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0155 - val_loss: 0.0592\n",
            "Epoch 463/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0173 - val_loss: 0.0689\n",
            "Epoch 464/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0158 - val_loss: 0.0923\n",
            "Epoch 465/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0172 - val_loss: 0.0665\n",
            "Epoch 466/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0180 - val_loss: 0.0903\n",
            "Epoch 467/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0166 - val_loss: 0.0582\n",
            "Epoch 468/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0156 - val_loss: 0.0931\n",
            "Epoch 469/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0163 - val_loss: 0.0721\n",
            "Epoch 470/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0156 - val_loss: 0.0891\n",
            "Epoch 471/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0138 - val_loss: 0.0695\n",
            "Epoch 472/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0142 - val_loss: 0.0793\n",
            "Epoch 473/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0142 - val_loss: 0.0767\n",
            "Epoch 474/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0141 - val_loss: 0.0744\n",
            "Epoch 475/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0133 - val_loss: 0.0896\n",
            "Epoch 476/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0141 - val_loss: 0.0664\n",
            "Epoch 477/600\n",
            "426/426 [==============================] - 0s 47us/sample - loss: 0.0162 - val_loss: 0.0801\n",
            "Epoch 478/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0153 - val_loss: 0.0878\n",
            "Epoch 479/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0132 - val_loss: 0.0659\n",
            "Epoch 480/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0140 - val_loss: 0.0784\n",
            "Epoch 481/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0143 - val_loss: 0.0841\n",
            "Epoch 482/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0145 - val_loss: 0.0686\n",
            "Epoch 483/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0134 - val_loss: 0.0944\n",
            "Epoch 484/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0133 - val_loss: 0.0743\n",
            "Epoch 485/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0147 - val_loss: 0.0727\n",
            "Epoch 486/600\n",
            "426/426 [==============================] - 0s 43us/sample - loss: 0.0132 - val_loss: 0.0934\n",
            "Epoch 487/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0139 - val_loss: 0.0754\n",
            "Epoch 488/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0129 - val_loss: 0.0827\n",
            "Epoch 489/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0133 - val_loss: 0.0822\n",
            "Epoch 490/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0126 - val_loss: 0.0693\n",
            "Epoch 491/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0137 - val_loss: 0.0895\n",
            "Epoch 492/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0132 - val_loss: 0.0757\n",
            "Epoch 493/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0126 - val_loss: 0.0779\n",
            "Epoch 494/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0126 - val_loss: 0.0805\n",
            "Epoch 495/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0136 - val_loss: 0.0771\n",
            "Epoch 496/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0121 - val_loss: 0.0900\n",
            "Epoch 497/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0125 - val_loss: 0.0753\n",
            "Epoch 498/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0126 - val_loss: 0.0858\n",
            "Epoch 499/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0125 - val_loss: 0.0799\n",
            "Epoch 500/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0133 - val_loss: 0.0888\n",
            "Epoch 501/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0133 - val_loss: 0.0719\n",
            "Epoch 502/600\n",
            "426/426 [==============================] - 0s 47us/sample - loss: 0.0124 - val_loss: 0.0976\n",
            "Epoch 503/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0124 - val_loss: 0.0803\n",
            "Epoch 504/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0119 - val_loss: 0.0823\n",
            "Epoch 505/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0121 - val_loss: 0.0881\n",
            "Epoch 506/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0123 - val_loss: 0.0867\n",
            "Epoch 507/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0116 - val_loss: 0.0768\n",
            "Epoch 508/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0121 - val_loss: 0.0859\n",
            "Epoch 509/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0118 - val_loss: 0.0802\n",
            "Epoch 510/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0119 - val_loss: 0.0861\n",
            "Epoch 511/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0114 - val_loss: 0.0774\n",
            "Epoch 512/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0120 - val_loss: 0.0851\n",
            "Epoch 513/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0120 - val_loss: 0.0866\n",
            "Epoch 514/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0115 - val_loss: 0.0775\n",
            "Epoch 515/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0116 - val_loss: 0.0897\n",
            "Epoch 516/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0118 - val_loss: 0.0808\n",
            "Epoch 517/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.0117 - val_loss: 0.0880\n",
            "Epoch 518/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0121 - val_loss: 0.0851\n",
            "Epoch 519/600\n",
            "426/426 [==============================] - 0s 45us/sample - loss: 0.0111 - val_loss: 0.0936\n",
            "Epoch 520/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0118 - val_loss: 0.0719\n",
            "Epoch 521/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0121 - val_loss: 0.0947\n",
            "Epoch 522/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0123 - val_loss: 0.0890\n",
            "Epoch 523/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.0111 - val_loss: 0.0809\n",
            "Epoch 524/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0114 - val_loss: 0.0855\n",
            "Epoch 525/600\n",
            "426/426 [==============================] - 0s 45us/sample - loss: 0.0113 - val_loss: 0.0929\n",
            "Epoch 526/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0118 - val_loss: 0.0821\n",
            "Epoch 527/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0103 - val_loss: 0.1009\n",
            "Epoch 528/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0111 - val_loss: 0.0877\n",
            "Epoch 529/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0105 - val_loss: 0.0811\n",
            "Epoch 530/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0108 - val_loss: 0.0922\n",
            "Epoch 531/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0105 - val_loss: 0.0885\n",
            "Epoch 532/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0121 - val_loss: 0.0825\n",
            "Epoch 533/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0108 - val_loss: 0.0978\n",
            "Epoch 534/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0121 - val_loss: 0.0745\n",
            "Epoch 535/600\n",
            "426/426 [==============================] - 0s 71us/sample - loss: 0.0120 - val_loss: 0.1113\n",
            "Epoch 536/600\n",
            "426/426 [==============================] - 0s 86us/sample - loss: 0.0129 - val_loss: 0.0739\n",
            "Epoch 537/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0151 - val_loss: 0.0979\n",
            "Epoch 538/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0115 - val_loss: 0.0726\n",
            "Epoch 539/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0123 - val_loss: 0.1034\n",
            "Epoch 540/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0110 - val_loss: 0.0843\n",
            "Epoch 541/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0107 - val_loss: 0.0959\n",
            "Epoch 542/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0104 - val_loss: 0.0863\n",
            "Epoch 543/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0100 - val_loss: 0.0925\n",
            "Epoch 544/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0100 - val_loss: 0.0860\n",
            "Epoch 545/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0098 - val_loss: 0.0939\n",
            "Epoch 546/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0100 - val_loss: 0.0906\n",
            "Epoch 547/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0101 - val_loss: 0.0896\n",
            "Epoch 548/600\n",
            "426/426 [==============================] - 0s 48us/sample - loss: 0.0096 - val_loss: 0.0945\n",
            "Epoch 549/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0112 - val_loss: 0.0830\n",
            "Epoch 550/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0113 - val_loss: 0.1048\n",
            "Epoch 551/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0096 - val_loss: 0.0734\n",
            "Epoch 552/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0105 - val_loss: 0.1002\n",
            "Epoch 553/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0103 - val_loss: 0.0912\n",
            "Epoch 554/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0108 - val_loss: 0.0864\n",
            "Epoch 555/600\n",
            "426/426 [==============================] - 0s 44us/sample - loss: 0.0095 - val_loss: 0.1067\n",
            "Epoch 556/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0096 - val_loss: 0.0843\n",
            "Epoch 557/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0107 - val_loss: 0.0923\n",
            "Epoch 558/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0098 - val_loss: 0.0998\n",
            "Epoch 559/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0099 - val_loss: 0.0883\n",
            "Epoch 560/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0094 - val_loss: 0.0967\n",
            "Epoch 561/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0092 - val_loss: 0.0910\n",
            "Epoch 562/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0089 - val_loss: 0.0854\n",
            "Epoch 563/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0094 - val_loss: 0.0933\n",
            "Epoch 564/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0100 - val_loss: 0.0960\n",
            "Epoch 565/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0091 - val_loss: 0.0897\n",
            "Epoch 566/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0092 - val_loss: 0.0968\n",
            "Epoch 567/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0087 - val_loss: 0.0862\n",
            "Epoch 568/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0091 - val_loss: 0.0921\n",
            "Epoch 569/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0090 - val_loss: 0.1004\n",
            "Epoch 570/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0089 - val_loss: 0.0867\n",
            "Epoch 571/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0089 - val_loss: 0.0902\n",
            "Epoch 572/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0090 - val_loss: 0.0954\n",
            "Epoch 573/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0089 - val_loss: 0.0848\n",
            "Epoch 574/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0086 - val_loss: 0.1043\n",
            "Epoch 575/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0091 - val_loss: 0.0922\n",
            "Epoch 576/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0085 - val_loss: 0.0952\n",
            "Epoch 577/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0092 - val_loss: 0.0950\n",
            "Epoch 578/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0091 - val_loss: 0.0976\n",
            "Epoch 579/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0088 - val_loss: 0.0949\n",
            "Epoch 580/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0086 - val_loss: 0.1057\n",
            "Epoch 581/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0083 - val_loss: 0.0893\n",
            "Epoch 582/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0087 - val_loss: 0.0975\n",
            "Epoch 583/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0086 - val_loss: 0.0922\n",
            "Epoch 584/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0089 - val_loss: 0.0998\n",
            "Epoch 585/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0090 - val_loss: 0.0893\n",
            "Epoch 586/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0092 - val_loss: 0.1047\n",
            "Epoch 587/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0090 - val_loss: 0.0821\n",
            "Epoch 588/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0091 - val_loss: 0.1098\n",
            "Epoch 589/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0088 - val_loss: 0.0948\n",
            "Epoch 590/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0079 - val_loss: 0.0988\n",
            "Epoch 591/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0079 - val_loss: 0.1014\n",
            "Epoch 592/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0080 - val_loss: 0.1005\n",
            "Epoch 593/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0079 - val_loss: 0.0974\n",
            "Epoch 594/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0081 - val_loss: 0.0918\n",
            "Epoch 595/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0081 - val_loss: 0.1039\n",
            "Epoch 596/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0079 - val_loss: 0.0893\n",
            "Epoch 597/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0079 - val_loss: 0.1003\n",
            "Epoch 598/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0079 - val_loss: 0.1002\n",
            "Epoch 599/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0082 - val_loss: 0.0963\n",
            "Epoch 600/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0076 - val_loss: 0.0996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f80e4d72160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGp-DVCondtN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "22b1e463-2770-4fa9-97a9-44e98d20e2c5"
      },
      "source": [
        "losses = pd.DataFrame(model.history.history)\n",
        "plt.figure(figsize=(15,5))\n",
        "losses.plot()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f80e4cad240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npQaI/wfp2cNqSv5lmAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ6-LpeeojOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EarlyStopping\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units=30, activation='relu'))\n",
        "model.add(Dense(units=15, activation='relu'))\n",
        "\n",
        "#Binary Classification\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0_tQUX9pGSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3w0UipFpNN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "985e09eb-16be-4882-bf28-0369af838563"
      },
      "source": [
        "help(EarlyStopping)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class EarlyStopping in module tensorflow.python.keras.callbacks:\n",
            "\n",
            "class EarlyStopping(Callback)\n",
            " |  Stop training when a monitored quantity has stopped improving.\n",
            " |  \n",
            " |  Arguments:\n",
            " |      monitor: Quantity to be monitored.\n",
            " |      min_delta: Minimum change in the monitored quantity\n",
            " |          to qualify as an improvement, i.e. an absolute\n",
            " |          change of less than min_delta, will count as no\n",
            " |          improvement.\n",
            " |      patience: Number of epochs with no improvement\n",
            " |          after which training will be stopped.\n",
            " |      verbose: verbosity mode.\n",
            " |      mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
            " |          training will stop when the quantity\n",
            " |          monitored has stopped decreasing; in `max`\n",
            " |          mode it will stop when the quantity\n",
            " |          monitored has stopped increasing; in `auto`\n",
            " |          mode, the direction is automatically inferred\n",
            " |          from the name of the monitored quantity.\n",
            " |      baseline: Baseline value for the monitored quantity.\n",
            " |          Training will stop if the model doesn't show improvement over the\n",
            " |          baseline.\n",
            " |      restore_best_weights: Whether to restore model weights from\n",
            " |          the epoch with the best value of the monitored quantity.\n",
            " |          If False, the model weights obtained at the last step of\n",
            " |          training are used.\n",
            " |  \n",
            " |  Example:\n",
            " |  \n",
            " |  ```python\n",
            " |  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
            " |  # This callback will stop the training when there is no improvement in\n",
            " |  # the validation loss for three consecutive epochs.\n",
            " |  model.fit(data, labels, epochs=100, callbacks=[callback],\n",
            " |      validation_data=(val_data, val_labels))\n",
            " |  ```\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      EarlyStopping\n",
            " |      Callback\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  get_monitor_value(self, logs)\n",
            " |  \n",
            " |  on_epoch_end(self, epoch, logs=None)\n",
            " |      Called at the end of an epoch.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run. This function should only\n",
            " |      be called during TRAIN mode.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          epoch: integer, index of epoch.\n",
            " |          logs: dict, metric results for this training epoch, and for the\n",
            " |            validation epoch if validation is performed. Validation result keys\n",
            " |            are prefixed with `val_`.\n",
            " |  \n",
            " |  on_train_begin(self, logs=None)\n",
            " |      Called at the beginning of training.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_train_end(self, logs=None)\n",
            " |      Called at the end of training.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from Callback:\n",
            " |  \n",
            " |  on_batch_begin(self, batch, logs=None)\n",
            " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
            " |  \n",
            " |  on_batch_end(self, batch, logs=None)\n",
            " |      A backwards compatibility alias for `on_train_batch_end`.\n",
            " |  \n",
            " |  on_epoch_begin(self, epoch, logs=None)\n",
            " |      Called at the start of an epoch.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run. This function should only\n",
            " |      be called during TRAIN mode.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          epoch: integer, index of epoch.\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_predict_batch_begin(self, batch, logs=None)\n",
            " |      Called at the beginning of a batch in `predict` methods.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
            " |            number and the size of the batch.\n",
            " |  \n",
            " |  on_predict_batch_end(self, batch, logs=None)\n",
            " |      Called at the end of a batch in `predict` methods.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Metric results for this batch.\n",
            " |  \n",
            " |  on_predict_begin(self, logs=None)\n",
            " |      Called at the beginning of prediction.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_predict_end(self, logs=None)\n",
            " |      Called at the end of prediction.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_test_batch_begin(self, batch, logs=None)\n",
            " |      Called at the beginning of a batch in `evaluate` methods.\n",
            " |      \n",
            " |      Also called at the beginning of a validation batch in the `fit`\n",
            " |      methods, if validation data is provided.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
            " |            number and the size of the batch.\n",
            " |  \n",
            " |  on_test_batch_end(self, batch, logs=None)\n",
            " |      Called at the end of a batch in `evaluate` methods.\n",
            " |      \n",
            " |      Also called at the end of a validation batch in the `fit`\n",
            " |      methods, if validation data is provided.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Metric results for this batch.\n",
            " |  \n",
            " |  on_test_begin(self, logs=None)\n",
            " |      Called at the beginning of evaluation or validation.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_test_end(self, logs=None)\n",
            " |      Called at the end of evaluation or validation.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_train_batch_begin(self, batch, logs=None)\n",
            " |      Called at the beginning of a training batch in `fit` methods.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
            " |            number and the size of the batch.\n",
            " |  \n",
            " |  on_train_batch_end(self, batch, logs=None)\n",
            " |      Called at the end of a training batch in `fit` methods.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Metric results for this batch.\n",
            " |  \n",
            " |  set_model(self, model)\n",
            " |  \n",
            " |  set_params(self, params)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from Callback:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK9VbR_opQYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=25, verbose=1, mode='min', restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqMpk5uyqK_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2ed5f2e-ea3b-4cbd-a298-79a4baeb7ffc"
      },
      "source": [
        "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test,y_test), batch_size=64, callbacks=[early_stop])"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 426 samples, validate on 143 samples\n",
            "Epoch 1/600\n",
            "426/426 [==============================] - 0s 1ms/sample - loss: 0.6672 - val_loss: 0.6692\n",
            "Epoch 2/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.6458 - val_loss: 0.6411\n",
            "Epoch 3/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.6214 - val_loss: 0.6136\n",
            "Epoch 4/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.5956 - val_loss: 0.5856\n",
            "Epoch 5/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.5697 - val_loss: 0.5607\n",
            "Epoch 6/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.5445 - val_loss: 0.5330\n",
            "Epoch 7/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.5172 - val_loss: 0.5012\n",
            "Epoch 8/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.4879 - val_loss: 0.4726\n",
            "Epoch 9/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.4586 - val_loss: 0.4419\n",
            "Epoch 10/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.4290 - val_loss: 0.4045\n",
            "Epoch 11/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.4043 - val_loss: 0.3778\n",
            "Epoch 12/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.3788 - val_loss: 0.3632\n",
            "Epoch 13/600\n",
            "426/426 [==============================] - 0s 74us/sample - loss: 0.3542 - val_loss: 0.3323\n",
            "Epoch 14/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.3339 - val_loss: 0.3072\n",
            "Epoch 15/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.3157 - val_loss: 0.2961\n",
            "Epoch 16/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.2981 - val_loss: 0.2787\n",
            "Epoch 17/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.2820 - val_loss: 0.2579\n",
            "Epoch 18/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.2695 - val_loss: 0.2471\n",
            "Epoch 19/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.2569 - val_loss: 0.2393\n",
            "Epoch 20/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.2449 - val_loss: 0.2216\n",
            "Epoch 21/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.2350 - val_loss: 0.2107\n",
            "Epoch 22/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.2254 - val_loss: 0.2001\n",
            "Epoch 23/600\n",
            "426/426 [==============================] - 0s 70us/sample - loss: 0.2176 - val_loss: 0.2024\n",
            "Epoch 24/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.2099 - val_loss: 0.1825\n",
            "Epoch 25/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.2017 - val_loss: 0.1770\n",
            "Epoch 26/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.1942 - val_loss: 0.1785\n",
            "Epoch 27/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.1890 - val_loss: 0.1671\n",
            "Epoch 28/600\n",
            "426/426 [==============================] - 0s 72us/sample - loss: 0.1821 - val_loss: 0.1584\n",
            "Epoch 29/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.1783 - val_loss: 0.1492\n",
            "Epoch 30/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.1710 - val_loss: 0.1515\n",
            "Epoch 31/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.1658 - val_loss: 0.1446\n",
            "Epoch 32/600\n",
            "426/426 [==============================] - 0s 73us/sample - loss: 0.1605 - val_loss: 0.1365\n",
            "Epoch 33/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.1562 - val_loss: 0.1321\n",
            "Epoch 34/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.1512 - val_loss: 0.1295\n",
            "Epoch 35/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.1471 - val_loss: 0.1276\n",
            "Epoch 36/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.1428 - val_loss: 0.1208\n",
            "Epoch 37/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.1386 - val_loss: 0.1138\n",
            "Epoch 38/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.1351 - val_loss: 0.1131\n",
            "Epoch 39/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.1345 - val_loss: 0.1178\n",
            "Epoch 40/600\n",
            "426/426 [==============================] - 0s 81us/sample - loss: 0.1274 - val_loss: 0.1020\n",
            "Epoch 41/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.1266 - val_loss: 0.1031\n",
            "Epoch 42/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.1222 - val_loss: 0.1021\n",
            "Epoch 43/600\n",
            "426/426 [==============================] - 0s 77us/sample - loss: 0.1195 - val_loss: 0.1005\n",
            "Epoch 44/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.1164 - val_loss: 0.0929\n",
            "Epoch 45/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.1135 - val_loss: 0.0943\n",
            "Epoch 46/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.1103 - val_loss: 0.0905\n",
            "Epoch 47/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.1095 - val_loss: 0.0865\n",
            "Epoch 48/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.1056 - val_loss: 0.0897\n",
            "Epoch 49/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.1041 - val_loss: 0.0874\n",
            "Epoch 50/600\n",
            "426/426 [==============================] - 0s 76us/sample - loss: 0.1022 - val_loss: 0.0798\n",
            "Epoch 51/600\n",
            "426/426 [==============================] - 0s 81us/sample - loss: 0.1001 - val_loss: 0.0823\n",
            "Epoch 52/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0978 - val_loss: 0.0825\n",
            "Epoch 53/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0960 - val_loss: 0.0784\n",
            "Epoch 54/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0949 - val_loss: 0.0758\n",
            "Epoch 55/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0945 - val_loss: 0.0797\n",
            "Epoch 56/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0914 - val_loss: 0.0715\n",
            "Epoch 57/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0913 - val_loss: 0.0726\n",
            "Epoch 58/600\n",
            "426/426 [==============================] - 0s 73us/sample - loss: 0.0878 - val_loss: 0.0696\n",
            "Epoch 59/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0864 - val_loss: 0.0689\n",
            "Epoch 60/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0863 - val_loss: 0.0703\n",
            "Epoch 61/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0839 - val_loss: 0.0662\n",
            "Epoch 62/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0830 - val_loss: 0.0671\n",
            "Epoch 63/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0819 - val_loss: 0.0655\n",
            "Epoch 64/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0808 - val_loss: 0.0674\n",
            "Epoch 65/600\n",
            "426/426 [==============================] - 0s 82us/sample - loss: 0.0793 - val_loss: 0.0631\n",
            "Epoch 66/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0803 - val_loss: 0.0606\n",
            "Epoch 67/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0781 - val_loss: 0.0687\n",
            "Epoch 68/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0776 - val_loss: 0.0617\n",
            "Epoch 69/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0770 - val_loss: 0.0600\n",
            "Epoch 70/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0763 - val_loss: 0.0667\n",
            "Epoch 71/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0747 - val_loss: 0.0592\n",
            "Epoch 72/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0738 - val_loss: 0.0593\n",
            "Epoch 73/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0730 - val_loss: 0.0571\n",
            "Epoch 74/600\n",
            "426/426 [==============================] - 0s 47us/sample - loss: 0.0725 - val_loss: 0.0619\n",
            "Epoch 75/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0713 - val_loss: 0.0573\n",
            "Epoch 76/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0714 - val_loss: 0.0559\n",
            "Epoch 77/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0702 - val_loss: 0.0610\n",
            "Epoch 78/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0712 - val_loss: 0.0577\n",
            "Epoch 79/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0687 - val_loss: 0.0529\n",
            "Epoch 80/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0687 - val_loss: 0.0567\n",
            "Epoch 81/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0673 - val_loss: 0.0608\n",
            "Epoch 82/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.0698 - val_loss: 0.0575\n",
            "Epoch 83/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0711 - val_loss: 0.0509\n",
            "Epoch 84/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0662 - val_loss: 0.0629\n",
            "Epoch 85/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0696 - val_loss: 0.0577\n",
            "Epoch 86/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0654 - val_loss: 0.0498\n",
            "Epoch 87/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0657 - val_loss: 0.0544\n",
            "Epoch 88/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0645 - val_loss: 0.0566\n",
            "Epoch 89/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0637 - val_loss: 0.0517\n",
            "Epoch 90/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0634 - val_loss: 0.0535\n",
            "Epoch 91/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0627 - val_loss: 0.0533\n",
            "Epoch 92/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0637 - val_loss: 0.0507\n",
            "Epoch 93/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0645 - val_loss: 0.0589\n",
            "Epoch 94/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0608 - val_loss: 0.0499\n",
            "Epoch 95/600\n",
            "426/426 [==============================] - 0s 41us/sample - loss: 0.0626 - val_loss: 0.0503\n",
            "Epoch 96/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0608 - val_loss: 0.0514\n",
            "Epoch 97/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0605 - val_loss: 0.0516\n",
            "Epoch 98/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0599 - val_loss: 0.0527\n",
            "Epoch 99/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0599 - val_loss: 0.0502\n",
            "Epoch 100/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0612 - val_loss: 0.0537\n",
            "Epoch 101/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0606 - val_loss: 0.0476\n",
            "Epoch 102/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0594 - val_loss: 0.0514\n",
            "Epoch 103/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0585 - val_loss: 0.0524\n",
            "Epoch 104/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0594 - val_loss: 0.0533\n",
            "Epoch 105/600\n",
            "426/426 [==============================] - 0s 79us/sample - loss: 0.0598 - val_loss: 0.0472\n",
            "Epoch 106/600\n",
            "426/426 [==============================] - 0s 73us/sample - loss: 0.0577 - val_loss: 0.0485\n",
            "Epoch 107/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0578 - val_loss: 0.0543\n",
            "Epoch 108/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0568 - val_loss: 0.0481\n",
            "Epoch 109/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0573 - val_loss: 0.0468\n",
            "Epoch 110/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0566 - val_loss: 0.0498\n",
            "Epoch 111/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0570 - val_loss: 0.0516\n",
            "Epoch 112/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0559 - val_loss: 0.0496\n",
            "Epoch 113/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0559 - val_loss: 0.0463\n",
            "Epoch 114/600\n",
            "426/426 [==============================] - 0s 47us/sample - loss: 0.0566 - val_loss: 0.0512\n",
            "Epoch 115/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0557 - val_loss: 0.0465\n",
            "Epoch 116/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.0564 - val_loss: 0.0458\n",
            "Epoch 117/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0543 - val_loss: 0.0523\n",
            "Epoch 118/600\n",
            "426/426 [==============================] - 0s 50us/sample - loss: 0.0583 - val_loss: 0.0529\n",
            "Epoch 119/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0562 - val_loss: 0.0438\n",
            "Epoch 120/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0553 - val_loss: 0.0513\n",
            "Epoch 121/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0540 - val_loss: 0.0501\n",
            "Epoch 122/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0536 - val_loss: 0.0475\n",
            "Epoch 123/600\n",
            "426/426 [==============================] - 0s 71us/sample - loss: 0.0537 - val_loss: 0.0487\n",
            "Epoch 124/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0533 - val_loss: 0.0456\n",
            "Epoch 125/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0537 - val_loss: 0.0465\n",
            "Epoch 126/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0529 - val_loss: 0.0527\n",
            "Epoch 127/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0534 - val_loss: 0.0451\n",
            "Epoch 128/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0526 - val_loss: 0.0489\n",
            "Epoch 129/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0533 - val_loss: 0.0441\n",
            "Epoch 130/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0518 - val_loss: 0.0500\n",
            "Epoch 131/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0523 - val_loss: 0.0491\n",
            "Epoch 132/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0522 - val_loss: 0.0444\n",
            "Epoch 133/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0522 - val_loss: 0.0471\n",
            "Epoch 134/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0529 - val_loss: 0.0436\n",
            "Epoch 135/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0528 - val_loss: 0.0521\n",
            "Epoch 136/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0506 - val_loss: 0.0457\n",
            "Epoch 137/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0510 - val_loss: 0.0436\n",
            "Epoch 138/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0508 - val_loss: 0.0453\n",
            "Epoch 139/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.0505 - val_loss: 0.0522\n",
            "Epoch 140/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0497 - val_loss: 0.0433\n",
            "Epoch 141/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.0512 - val_loss: 0.0438\n",
            "Epoch 142/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0526 - val_loss: 0.0577\n",
            "Epoch 143/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0501 - val_loss: 0.0417\n",
            "Epoch 144/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0505 - val_loss: 0.0475\n",
            "Epoch 145/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0486 - val_loss: 0.0479\n",
            "Epoch 146/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0484 - val_loss: 0.0464\n",
            "Epoch 147/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0489 - val_loss: 0.0427\n",
            "Epoch 148/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0501 - val_loss: 0.0520\n",
            "Epoch 149/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0498 - val_loss: 0.0418\n",
            "Epoch 150/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0473 - val_loss: 0.0495\n",
            "Epoch 151/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0480 - val_loss: 0.0489\n",
            "Epoch 152/600\n",
            "426/426 [==============================] - 0s 102us/sample - loss: 0.0478 - val_loss: 0.0481\n",
            "Epoch 153/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0464 - val_loss: 0.0429\n",
            "Epoch 154/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0469 - val_loss: 0.0450\n",
            "Epoch 155/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0462 - val_loss: 0.0483\n",
            "Epoch 156/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0460 - val_loss: 0.0454\n",
            "Epoch 157/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0462 - val_loss: 0.0472\n",
            "Epoch 158/600\n",
            "426/426 [==============================] - 0s 81us/sample - loss: 0.0459 - val_loss: 0.0443\n",
            "Epoch 159/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.0459 - val_loss: 0.0477\n",
            "Epoch 160/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0454 - val_loss: 0.0452\n",
            "Epoch 161/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0456 - val_loss: 0.0420\n",
            "Epoch 162/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.0458 - val_loss: 0.0456\n",
            "Epoch 163/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0447 - val_loss: 0.0460\n",
            "Epoch 164/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0448 - val_loss: 0.0465\n",
            "Epoch 165/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0454 - val_loss: 0.0432\n",
            "Epoch 166/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.0478 - val_loss: 0.0521\n",
            "Epoch 167/600\n",
            "426/426 [==============================] - 0s 52us/sample - loss: 0.0439 - val_loss: 0.0423\n",
            "Epoch 168/600\n",
            " 64/426 [===>..........................] - ETA: 0s - loss: 0.0602Restoring model weights from the end of the best epoch.\n",
            "426/426 [==============================] - 0s 326us/sample - loss: 0.0442 - val_loss: 0.0430\n",
            "Epoch 00168: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f80e4cbad68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15DAON2Mqedi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "4d92e297-50af-402f-fd66-82d645e457ea"
      },
      "source": [
        "losses = pd.DataFrame(model.history.history)\n",
        "plt.figure(figsize=(15,5))\n",
        "losses.plot()"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f80e4b1bfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDXSllOoiNNCVUqqL0EBXSqku4v8BJKJCgC1ostMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-udoIwFqk6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dropout Layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units=30, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(units=15, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "#Binary Classification\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWCUxds0rY2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ced11a68-19b0-4c51-d544-39b62b8fef1b"
      },
      "source": [
        "model.fit(x=X_train, y=y_train, epochs=600, validation_data=(X_test,y_test), batch_size=64, callbacks=[early_stop])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 426 samples, validate on 143 samples\n",
            "Epoch 1/600\n",
            "426/426 [==============================] - 1s 2ms/sample - loss: 0.7040 - val_loss: 0.6902\n",
            "Epoch 2/600\n",
            "426/426 [==============================] - 0s 74us/sample - loss: 0.6797 - val_loss: 0.6788\n",
            "Epoch 3/600\n",
            "426/426 [==============================] - 0s 108us/sample - loss: 0.6692 - val_loss: 0.6694\n",
            "Epoch 4/600\n",
            "426/426 [==============================] - 0s 71us/sample - loss: 0.6511 - val_loss: 0.6610\n",
            "Epoch 5/600\n",
            "426/426 [==============================] - 0s 82us/sample - loss: 0.6539 - val_loss: 0.6532\n",
            "Epoch 6/600\n",
            "426/426 [==============================] - 0s 77us/sample - loss: 0.6394 - val_loss: 0.6450\n",
            "Epoch 7/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.6283 - val_loss: 0.6360\n",
            "Epoch 8/600\n",
            "426/426 [==============================] - 0s 82us/sample - loss: 0.6259 - val_loss: 0.6254\n",
            "Epoch 9/600\n",
            "426/426 [==============================] - 0s 76us/sample - loss: 0.6069 - val_loss: 0.6093\n",
            "Epoch 10/600\n",
            "426/426 [==============================] - 0s 88us/sample - loss: 0.5998 - val_loss: 0.5892\n",
            "Epoch 11/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.5830 - val_loss: 0.5636\n",
            "Epoch 12/600\n",
            "426/426 [==============================] - 0s 75us/sample - loss: 0.5544 - val_loss: 0.5324\n",
            "Epoch 13/600\n",
            "426/426 [==============================] - 0s 78us/sample - loss: 0.5375 - val_loss: 0.5017\n",
            "Epoch 14/600\n",
            "426/426 [==============================] - 0s 71us/sample - loss: 0.5247 - val_loss: 0.4709\n",
            "Epoch 15/600\n",
            "426/426 [==============================] - 0s 71us/sample - loss: 0.4847 - val_loss: 0.4396\n",
            "Epoch 16/600\n",
            "426/426 [==============================] - 0s 75us/sample - loss: 0.4679 - val_loss: 0.4109\n",
            "Epoch 17/600\n",
            "426/426 [==============================] - 0s 78us/sample - loss: 0.4691 - val_loss: 0.3896\n",
            "Epoch 18/600\n",
            "426/426 [==============================] - 0s 73us/sample - loss: 0.4346 - val_loss: 0.3773\n",
            "Epoch 19/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.4394 - val_loss: 0.3607\n",
            "Epoch 20/600\n",
            "426/426 [==============================] - 0s 70us/sample - loss: 0.4059 - val_loss: 0.3371\n",
            "Epoch 21/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.3629 - val_loss: 0.3182\n",
            "Epoch 22/600\n",
            "426/426 [==============================] - 0s 51us/sample - loss: 0.3626 - val_loss: 0.3018\n",
            "Epoch 23/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.3623 - val_loss: 0.2918\n",
            "Epoch 24/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.3297 - val_loss: 0.2813\n",
            "Epoch 25/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.3389 - val_loss: 0.2616\n",
            "Epoch 26/600\n",
            "426/426 [==============================] - 0s 79us/sample - loss: 0.3183 - val_loss: 0.2476\n",
            "Epoch 27/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.3117 - val_loss: 0.2310\n",
            "Epoch 28/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.2968 - val_loss: 0.2226\n",
            "Epoch 29/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.3105 - val_loss: 0.2162\n",
            "Epoch 30/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.2943 - val_loss: 0.2141\n",
            "Epoch 31/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.3010 - val_loss: 0.1998\n",
            "Epoch 32/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.2667 - val_loss: 0.1882\n",
            "Epoch 33/600\n",
            "426/426 [==============================] - 0s 70us/sample - loss: 0.2517 - val_loss: 0.1858\n",
            "Epoch 34/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.2588 - val_loss: 0.1851\n",
            "Epoch 35/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.2463 - val_loss: 0.1708\n",
            "Epoch 36/600\n",
            "426/426 [==============================] - 0s 70us/sample - loss: 0.2443 - val_loss: 0.1611\n",
            "Epoch 37/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.2338 - val_loss: 0.1570\n",
            "Epoch 38/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.2393 - val_loss: 0.1549\n",
            "Epoch 39/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.2280 - val_loss: 0.1556\n",
            "Epoch 40/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.2274 - val_loss: 0.1498\n",
            "Epoch 41/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.2174 - val_loss: 0.1391\n",
            "Epoch 42/600\n",
            "426/426 [==============================] - 0s 71us/sample - loss: 0.1910 - val_loss: 0.1391\n",
            "Epoch 43/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.2149 - val_loss: 0.1352\n",
            "Epoch 44/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.2123 - val_loss: 0.1308\n",
            "Epoch 45/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.1941 - val_loss: 0.1328\n",
            "Epoch 46/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.2192 - val_loss: 0.1246\n",
            "Epoch 47/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.1978 - val_loss: 0.1205\n",
            "Epoch 48/600\n",
            "426/426 [==============================] - 0s 74us/sample - loss: 0.1722 - val_loss: 0.1213\n",
            "Epoch 49/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.1915 - val_loss: 0.1166\n",
            "Epoch 50/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.1867 - val_loss: 0.1135\n",
            "Epoch 51/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.1981 - val_loss: 0.1087\n",
            "Epoch 52/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.1811 - val_loss: 0.1089\n",
            "Epoch 53/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.1723 - val_loss: 0.1110\n",
            "Epoch 54/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.1707 - val_loss: 0.1025\n",
            "Epoch 55/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.1620 - val_loss: 0.0966\n",
            "Epoch 56/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.1544 - val_loss: 0.0957\n",
            "Epoch 57/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.1751 - val_loss: 0.0949\n",
            "Epoch 58/600\n",
            "426/426 [==============================] - 0s 70us/sample - loss: 0.1548 - val_loss: 0.0924\n",
            "Epoch 59/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.1796 - val_loss: 0.0909\n",
            "Epoch 60/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.1720 - val_loss: 0.0928\n",
            "Epoch 61/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.1486 - val_loss: 0.0928\n",
            "Epoch 62/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.1608 - val_loss: 0.0884\n",
            "Epoch 63/600\n",
            "426/426 [==============================] - 0s 72us/sample - loss: 0.1478 - val_loss: 0.0887\n",
            "Epoch 64/600\n",
            "426/426 [==============================] - 0s 75us/sample - loss: 0.1516 - val_loss: 0.0852\n",
            "Epoch 65/600\n",
            "426/426 [==============================] - 0s 89us/sample - loss: 0.1434 - val_loss: 0.0830\n",
            "Epoch 66/600\n",
            "426/426 [==============================] - 0s 70us/sample - loss: 0.1485 - val_loss: 0.0812\n",
            "Epoch 67/600\n",
            "426/426 [==============================] - 0s 74us/sample - loss: 0.1409 - val_loss: 0.0796\n",
            "Epoch 68/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.1543 - val_loss: 0.0814\n",
            "Epoch 69/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.1580 - val_loss: 0.0837\n",
            "Epoch 70/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.1420 - val_loss: 0.0743\n",
            "Epoch 71/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.1310 - val_loss: 0.0728\n",
            "Epoch 72/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.1510 - val_loss: 0.0739\n",
            "Epoch 73/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.1640 - val_loss: 0.0827\n",
            "Epoch 74/600\n",
            "426/426 [==============================] - 0s 80us/sample - loss: 0.1463 - val_loss: 0.0810\n",
            "Epoch 75/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.1183 - val_loss: 0.0716\n",
            "Epoch 76/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.1200 - val_loss: 0.0689\n",
            "Epoch 77/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.1331 - val_loss: 0.0696\n",
            "Epoch 78/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.1386 - val_loss: 0.0748\n",
            "Epoch 79/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.1504 - val_loss: 0.0690\n",
            "Epoch 80/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.1396 - val_loss: 0.0643\n",
            "Epoch 81/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.1261 - val_loss: 0.0656\n",
            "Epoch 82/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.1340 - val_loss: 0.0740\n",
            "Epoch 83/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.1164 - val_loss: 0.0656\n",
            "Epoch 84/600\n",
            "426/426 [==============================] - 0s 74us/sample - loss: 0.1173 - val_loss: 0.0620\n",
            "Epoch 85/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.1225 - val_loss: 0.0616\n",
            "Epoch 86/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.1150 - val_loss: 0.0649\n",
            "Epoch 87/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.1287 - val_loss: 0.0631\n",
            "Epoch 88/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.1000 - val_loss: 0.0602\n",
            "Epoch 89/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.1204 - val_loss: 0.0595\n",
            "Epoch 90/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.1222 - val_loss: 0.0598\n",
            "Epoch 91/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.1320 - val_loss: 0.0594\n",
            "Epoch 92/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.1124 - val_loss: 0.0584\n",
            "Epoch 93/600\n",
            "426/426 [==============================] - 0s 80us/sample - loss: 0.1213 - val_loss: 0.0578\n",
            "Epoch 94/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.1073 - val_loss: 0.0582\n",
            "Epoch 95/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.1081 - val_loss: 0.0630\n",
            "Epoch 96/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.1071 - val_loss: 0.0599\n",
            "Epoch 97/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.1043 - val_loss: 0.0595\n",
            "Epoch 98/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.1097 - val_loss: 0.0557\n",
            "Epoch 99/600\n",
            "426/426 [==============================] - 0s 73us/sample - loss: 0.1078 - val_loss: 0.0597\n",
            "Epoch 100/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.1126 - val_loss: 0.0566\n",
            "Epoch 101/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.1011 - val_loss: 0.0549\n",
            "Epoch 102/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.1085 - val_loss: 0.0545\n",
            "Epoch 103/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.1049 - val_loss: 0.0549\n",
            "Epoch 104/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.1193 - val_loss: 0.0562\n",
            "Epoch 105/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0950 - val_loss: 0.0565\n",
            "Epoch 106/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.1023 - val_loss: 0.0561\n",
            "Epoch 107/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0952 - val_loss: 0.0596\n",
            "Epoch 108/600\n",
            "426/426 [==============================] - 0s 79us/sample - loss: 0.1303 - val_loss: 0.0531\n",
            "Epoch 109/600\n",
            "426/426 [==============================] - 0s 73us/sample - loss: 0.0847 - val_loss: 0.0509\n",
            "Epoch 110/600\n",
            "426/426 [==============================] - 0s 77us/sample - loss: 0.1042 - val_loss: 0.0520\n",
            "Epoch 111/600\n",
            "426/426 [==============================] - 0s 79us/sample - loss: 0.0899 - val_loss: 0.0521\n",
            "Epoch 112/600\n",
            "426/426 [==============================] - 0s 71us/sample - loss: 0.1055 - val_loss: 0.0519\n",
            "Epoch 113/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.1082 - val_loss: 0.0503\n",
            "Epoch 114/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.1083 - val_loss: 0.0515\n",
            "Epoch 115/600\n",
            "426/426 [==============================] - 0s 82us/sample - loss: 0.0966 - val_loss: 0.0523\n",
            "Epoch 116/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.1097 - val_loss: 0.0504\n",
            "Epoch 117/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0963 - val_loss: 0.0533\n",
            "Epoch 118/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.1047 - val_loss: 0.0499\n",
            "Epoch 119/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.1114 - val_loss: 0.0547\n",
            "Epoch 120/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.1012 - val_loss: 0.0524\n",
            "Epoch 121/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0913 - val_loss: 0.0476\n",
            "Epoch 122/600\n",
            "426/426 [==============================] - 0s 49us/sample - loss: 0.1092 - val_loss: 0.0517\n",
            "Epoch 123/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0866 - val_loss: 0.0482\n",
            "Epoch 124/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0838 - val_loss: 0.0454\n",
            "Epoch 125/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0860 - val_loss: 0.0459\n",
            "Epoch 126/600\n",
            "426/426 [==============================] - 0s 65us/sample - loss: 0.0948 - val_loss: 0.0462\n",
            "Epoch 127/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0891 - val_loss: 0.0471\n",
            "Epoch 128/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0915 - val_loss: 0.0501\n",
            "Epoch 129/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0819 - val_loss: 0.0455\n",
            "Epoch 130/600\n",
            "426/426 [==============================] - 0s 79us/sample - loss: 0.1005 - val_loss: 0.0443\n",
            "Epoch 131/600\n",
            "426/426 [==============================] - 0s 78us/sample - loss: 0.0957 - val_loss: 0.0521\n",
            "Epoch 132/600\n",
            "426/426 [==============================] - 0s 71us/sample - loss: 0.0879 - val_loss: 0.0534\n",
            "Epoch 133/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0932 - val_loss: 0.0461\n",
            "Epoch 134/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0908 - val_loss: 0.0437\n",
            "Epoch 135/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.1085 - val_loss: 0.0446\n",
            "Epoch 136/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0748 - val_loss: 0.0468\n",
            "Epoch 137/600\n",
            "426/426 [==============================] - 0s 54us/sample - loss: 0.1041 - val_loss: 0.0526\n",
            "Epoch 138/600\n",
            "426/426 [==============================] - 0s 55us/sample - loss: 0.1000 - val_loss: 0.0520\n",
            "Epoch 139/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.1083 - val_loss: 0.0480\n",
            "Epoch 140/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0887 - val_loss: 0.0487\n",
            "Epoch 141/600\n",
            "426/426 [==============================] - 0s 64us/sample - loss: 0.0807 - val_loss: 0.0464\n",
            "Epoch 142/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0959 - val_loss: 0.0460\n",
            "Epoch 143/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0817 - val_loss: 0.0455\n",
            "Epoch 144/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0844 - val_loss: 0.0464\n",
            "Epoch 145/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0760 - val_loss: 0.0456\n",
            "Epoch 146/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0705 - val_loss: 0.0435\n",
            "Epoch 147/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.0765 - val_loss: 0.0436\n",
            "Epoch 148/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0686 - val_loss: 0.0401\n",
            "Epoch 149/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0774 - val_loss: 0.0395\n",
            "Epoch 150/600\n",
            "426/426 [==============================] - 0s 61us/sample - loss: 0.0943 - val_loss: 0.0405\n",
            "Epoch 151/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0762 - val_loss: 0.0485\n",
            "Epoch 152/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0847 - val_loss: 0.0428\n",
            "Epoch 153/600\n",
            "426/426 [==============================] - 0s 62us/sample - loss: 0.0915 - val_loss: 0.0415\n",
            "Epoch 154/600\n",
            "426/426 [==============================] - 0s 57us/sample - loss: 0.0898 - val_loss: 0.0449\n",
            "Epoch 155/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.0858 - val_loss: 0.0427\n",
            "Epoch 156/600\n",
            "426/426 [==============================] - 0s 74us/sample - loss: 0.0803 - val_loss: 0.0448\n",
            "Epoch 157/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0825 - val_loss: 0.0449\n",
            "Epoch 158/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0758 - val_loss: 0.0409\n",
            "Epoch 159/600\n",
            "426/426 [==============================] - 0s 63us/sample - loss: 0.0680 - val_loss: 0.0406\n",
            "Epoch 160/600\n",
            "426/426 [==============================] - 0s 66us/sample - loss: 0.0939 - val_loss: 0.0454\n",
            "Epoch 161/600\n",
            "426/426 [==============================] - 0s 68us/sample - loss: 0.0825 - val_loss: 0.0505\n",
            "Epoch 162/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0881 - val_loss: 0.0455\n",
            "Epoch 163/600\n",
            "426/426 [==============================] - 0s 59us/sample - loss: 0.0747 - val_loss: 0.0448\n",
            "Epoch 164/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0606 - val_loss: 0.0412\n",
            "Epoch 165/600\n",
            "426/426 [==============================] - 0s 70us/sample - loss: 0.0665 - val_loss: 0.0426\n",
            "Epoch 166/600\n",
            "426/426 [==============================] - 0s 58us/sample - loss: 0.0836 - val_loss: 0.0468\n",
            "Epoch 167/600\n",
            "426/426 [==============================] - 0s 77us/sample - loss: 0.0872 - val_loss: 0.0483\n",
            "Epoch 168/600\n",
            "426/426 [==============================] - 0s 69us/sample - loss: 0.0906 - val_loss: 0.0426\n",
            "Epoch 169/600\n",
            "426/426 [==============================] - 0s 67us/sample - loss: 0.0879 - val_loss: 0.0412\n",
            "Epoch 170/600\n",
            "426/426 [==============================] - 0s 60us/sample - loss: 0.0805 - val_loss: 0.0471\n",
            "Epoch 171/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0688 - val_loss: 0.0454\n",
            "Epoch 172/600\n",
            "426/426 [==============================] - 0s 56us/sample - loss: 0.0909 - val_loss: 0.0423\n",
            "Epoch 173/600\n",
            "426/426 [==============================] - 0s 53us/sample - loss: 0.0870 - val_loss: 0.0418\n",
            "Epoch 174/600\n",
            " 64/426 [===>..........................] - ETA: 0s - loss: 0.1007Restoring model weights from the end of the best epoch.\n",
            "426/426 [==============================] - 0s 420us/sample - loss: 0.0821 - val_loss: 0.0414\n",
            "Epoch 00174: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f80e49999b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVBiiGkKsLxt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "282f2d00-adc8-4562-f638-8becc575f312"
      },
      "source": [
        "losses = pd.DataFrame(model.history.history)\n",
        "plt.figure(figsize=(15,5))\n",
        "losses.plot()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f80e494bc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf0IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DQ6H7tOsOym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYvSGLxosowt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_6ntTXEstqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "66eab986-01ee-4dd6-d78a-58216a69bbd9"
      },
      "source": [
        "print(classification_report(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98        45\n",
            "           1       0.99      0.99      0.99        98\n",
            "\n",
            "    accuracy                           0.99       143\n",
            "   macro avg       0.98      0.98      0.98       143\n",
            "weighted avg       0.99      0.99      0.99       143\n",
            "\n",
            "[[44  1]\n",
            " [ 1 97]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO7ISovds12Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('best_cancer_detector.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELHccu7btAp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}